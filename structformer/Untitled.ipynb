{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4efdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from structformer.data.tokenizer import Tokenizer\n",
    "from structformer.evaluation.test_object_selection_network import ObjectSelectionInference\n",
    "from structformer.evaluation.test_structformer import PriorInference\n",
    "from structformer.utils.rearrangement import show_pcs_with_predictions, get_initial_scene_idxs, evaluate_target_object_predictions, save_img, show_pcs_with_labels, test_new_vis, show_pcs\n",
    "from structformer.evaluation.inference import PointCloudRearrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb388f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# point cloud utils\n",
    "from pc_utils import depth2pc\n",
    "\n",
    "# tabletop environment\n",
    "FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(os.path.join(FILE_PATH, '../..', 'TabletopTidyingUp/pybullet_ur5_robotiq'))\n",
    "from custom_env import TableTopTidyingUpEnv, get_contact_objects\n",
    "from utilities import Camera, Camera_front_top\n",
    "sys.path.append(os.path.join(FILE_PATH, '../..', 'TabletopTidyingUp'))\n",
    "from collect_template_list import scene_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupEnvironment(args=None):\n",
    "    camera_top = Camera((0, 0, 1.45), 0.02, 2, (480, 360), 60)\n",
    "    camera_front_top = Camera_front_top((0.5, 0, 1.3), 0.02, 2, (480, 360), 60)\n",
    "    \n",
    "    data_dir = '/ssd/disk' #args.data_dir\n",
    "    objects_cfg = { 'paths': {\n",
    "            'pybullet_object_path' : os.path.join(data_dir, 'pybullet-URDF-models/urdf_models/models'),\n",
    "            'ycb_object_path' : os.path.join(data_dir, 'YCB_dataset'),\n",
    "            'housecat_object_path' : os.path.join(data_dir, 'housecat6d/obj_models_small_size_final'),\n",
    "        },\n",
    "        'split' : 'inference' #args.object_split #'inference' #'train'\n",
    "    }\n",
    "    \n",
    "    gui_on = not args.gui_off\n",
    "    env = TableTopTidyingUpEnv(objects_cfg, camera_top, camera_front_top, vis=gui_on, gripper_type='85')\n",
    "    p.resetDebugVisualizerCamera(2.0, -270., -60., (0., 0., 0.))\n",
    "    p.configureDebugVisualizer(p.COV_ENABLE_SHADOWS, 1)  # Shadows on/off\n",
    "    p.addUserDebugLine([0, -0.5, 0], [0, -0.5, 1.1], [0, 1, 0])\n",
    "\n",
    "    env.reset()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b043e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = setupEnvironment()\n",
    "\n",
    "scenes = [s for s in sorted(list(scene_list.keys())) if s.startswith('D')]\n",
    "selected_scene = random.choice(scenes)\n",
    "print('Selected scene: %s' %selected_scene)\n",
    "\n",
    "objects = scene_list[selected_scene]\n",
    "\n",
    "sizes = []\n",
    "for i in range(len(objects)):\n",
    "    if 'small' in objects[i]:\n",
    "        sizes.append('small')\n",
    "        objects[i] = objects[i].replace('small_', '')\n",
    "    elif 'large' in objects[i]:\n",
    "        sizes.append('large')\n",
    "        objects[i] = objects[i].replace('large_', '')\n",
    "    else:\n",
    "        sizes.append('medium')\n",
    "objects = [[objects[i], sizes[i]] for i in range(len(objects))]\n",
    "selected_objects = objects\n",
    "\n",
    "env.spawn_objects(selected_objects)\n",
    "env.arrange_objects(random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cf82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "rgb = obs['top']['rgb']\n",
    "depth = obs['top']['depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e22a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = ''\n",
    "object_selection_model_dir = ''\n",
    "pose_generation_model_dir = ''\n",
    "dirs_config = ''\n",
    "\n",
    "dirs_cfg = OmegaConf.load(dirs_config)\n",
    "dirs_cfg.dataset_base_dir = dataset_base_dir\n",
    "OmegaConf.resolve(dirs_cfg)\n",
    "\n",
    "# load models\n",
    "object_selection_inference = ObjectSelectionInference(object_selection_model_dir, dirs_cfg)\n",
    "pose_generation_inference = PriorInference(pose_generation_model_dir, dirs_cfg)\n",
    "\n",
    "test_dataset = object_selection_inference.dataset\n",
    "initial_scene_idxs = get_initial_scene_idxs(test_dataset)\n",
    "\n",
    "idx = 0\n",
    "#for idx in range(len(test_dataset)):\n",
    "#    if idx not in initial_scene_idxs:\n",
    "#        continue\n",
    "#    if idx == 4:\n",
    "#        continue\n",
    "\n",
    "filename, _ = test_dataset.get_data_index(idx)\n",
    "scene_id = os.path.split(filename)[1][4:-3]\n",
    "print(\"-\"*50)\n",
    "print(\"Scene No.{}\".format(scene_id))\n",
    "\n",
    "# retrieve data\n",
    "init_datum = test_dataset.get_raw_data(idx)\n",
    "goal_specification = init_datum[\"goal_specification\"]\n",
    "object_selection_structured_sentence = init_datum[\"sentence\"][5:]\n",
    "structure_specification_structured_sentence = init_datum[\"sentence\"][:5]\n",
    "object_selection_natural_sentence = object_selection_inference.tokenizer.convert_to_natural_sentence(\n",
    "    object_selection_structured_sentence)\n",
    "structure_specification_natural_sentence = object_selection_inference.tokenizer.convert_structure_params_to_natural_language(structure_specification_structured_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df572599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object selection\n",
    "predictions, gts = object_selection_inference.predict_target_objects(init_datum)\n",
    "\n",
    "all_obj_xyzs = init_datum[\"xyzs\"][:len(predictions)]\n",
    "all_obj_rgbs = init_datum[\"rgbs\"][:len(predictions)]\n",
    "obj_idxs = [i for i, l in enumerate(predictions) if l == 1.0]\n",
    "if len(obj_idxs) == 0:\n",
    "    continue\n",
    "other_obj_idxs = [i for i, l in enumerate(predictions) if l == 0.0]\n",
    "obj_xyzs = [all_obj_xyzs[i] for i in obj_idxs]\n",
    "obj_rgbs = [all_obj_rgbs[i] for i in obj_idxs]\n",
    "other_obj_xyzs = [all_obj_xyzs[i] for i in other_obj_idxs]\n",
    "other_obj_rgbs = [all_obj_rgbs[i] for i in other_obj_idxs]\n",
    "\n",
    "print(\"\\nSelect objects to rearrange...\")\n",
    "print(\"Instruction:\", object_selection_natural_sentence)\n",
    "print(\"Visualize groundtruth (dot color) and prediction (ring color)\")\n",
    "show_pcs_with_predictions(init_datum[\"xyzs\"][:len(predictions)], init_datum[\"rgbs\"][:len(predictions)],\n",
    "                          gts, predictions, add_table=True, side_view=True)\n",
    "print(\"Visualize object to rearrange\")\n",
    "show_pcs(obj_xyzs, obj_rgbs, side_view=True, add_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose generation\n",
    "max_num_objects = pose_generation_inference.cfg.dataset.max_num_objects\n",
    "max_num_other_objects = pose_generation_inference.cfg.dataset.max_num_other_objects\n",
    "if len(obj_xyzs) > max_num_objects:\n",
    "    print(\"WARNING: reducing the number of \\\"query\\\" objects because this model is trained with a maximum of {} \\\"query\\\" objects. Train a new model if a larger number is needed.\".format(max_num_objects))\n",
    "    obj_xyzs = obj_xyzs[:max_num_objects]\n",
    "    obj_rgbs = obj_rgbs[:max_num_objects]\n",
    "if len(other_obj_xyzs) > max_num_other_objects:\n",
    "    print(\"WARNING: reducing the number of \\\"distractor\\\" objects because this model is trained with a maximum of {} \\\"distractor\\\" objects. Train a new model if a larger number is needed.\".format(max_num_other_objects))\n",
    "    other_obj_xyzs = other_obj_xyzs[:max_num_other_objects]\n",
    "    other_obj_rgbs = other_obj_rgbs[:max_num_other_objects]\n",
    "\n",
    "pose_generation_datum = pose_generation_inference.dataset.prepare_test_data(obj_xyzs, obj_rgbs,\n",
    "                                                                            other_obj_xyzs, other_obj_rgbs,\n",
    "                                                                            goal_specification[\"shape\"])\n",
    "beam_data = []\n",
    "beam_pc_rearrangements = []\n",
    "for b in range(beam_size):\n",
    "    datum_copy = copy.deepcopy(pose_generation_datum)\n",
    "    beam_data.append(datum_copy)\n",
    "    beam_pc_rearrangements.append(PointCloudRearrangement(datum_copy))\n",
    "\n",
    "# autoregressive decoding\n",
    "num_target_objects = beam_pc_rearrangements[0].num_target_objects\n",
    "\n",
    "# first predict structure pose\n",
    "beam_goal_struct_pose, target_object_preds = pose_generation_inference.limited_batch_inference(beam_data)\n",
    "for b in range(beam_size):\n",
    "    datum = beam_data[b]\n",
    "    datum[\"struct_x_inputs\"] = [beam_goal_struct_pose[b][0]]\n",
    "    datum[\"struct_y_inputs\"] = [beam_goal_struct_pose[b][1]]\n",
    "    datum[\"struct_z_inputs\"] = [beam_goal_struct_pose[b][2]]\n",
    "    datum[\"struct_theta_inputs\"] = [beam_goal_struct_pose[b][3:]]\n",
    "\n",
    "# then iteratively predict pose of each object\n",
    "beam_goal_obj_poses = []\n",
    "for obj_idx in range(num_target_objects):\n",
    "    struct_preds, target_object_preds = pose_generation_inference.limited_batch_inference(beam_data)\n",
    "    beam_goal_obj_poses.append(target_object_preds[:, obj_idx])\n",
    "    for b in range(beam_size):\n",
    "        datum = beam_data[b]\n",
    "        datum[\"obj_x_inputs\"][obj_idx] = target_object_preds[b][obj_idx][0]\n",
    "        datum[\"obj_y_inputs\"][obj_idx] = target_object_preds[b][obj_idx][1]\n",
    "        datum[\"obj_z_inputs\"][obj_idx] = target_object_preds[b][obj_idx][2]\n",
    "        datum[\"obj_theta_inputs\"][obj_idx] = target_object_preds[b][obj_idx][3:]\n",
    "# concat in the object dim\n",
    "beam_goal_obj_poses = np.stack(beam_goal_obj_poses, axis=0)\n",
    "# swap axis\n",
    "beam_goal_obj_poses = np.swapaxes(beam_goal_obj_poses, 1, 0)  # batch size, number of target objects, pose dim\n",
    "\n",
    "# move pc\n",
    "for bi in range(beam_size):\n",
    "    beam_pc_rearrangements[bi].set_goal_poses(beam_goal_struct_pose[bi], beam_goal_obj_poses[bi])\n",
    "    beam_pc_rearrangements[bi].rearrange()\n",
    "\n",
    "print(\"\\nRearrange \\\"query\\\" objects...\")\n",
    "print(\"Instruction:\", structure_specification_natural_sentence)\n",
    "for pi, pc_rearrangement in enumerate(beam_pc_rearrangements):\n",
    "    print(\"Visualize rearranged scene sample {}\".format(pi))\n",
    "    pc_rearrangement.visualize(\"goal\", add_other_objects=True, add_table=True, side_view=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
